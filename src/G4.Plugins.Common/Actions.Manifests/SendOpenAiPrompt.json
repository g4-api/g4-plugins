{
	"author": {
		"link": "https://www.linkedin.com/in/roei-sabag-247aa18/",
		"name": "Roei Sabag"
	},
	"categories": [
		"OpenAI"
	],
	"context": {
		"integration": {
			"github": {
				"document": "https://github.com/g4-api/g4-plugins/blob/main/docs/Actions/SendOpenAiPrompt.md",
				"source": "https://github.com/g4-api/g4-plugins/blob/main/src/G4.Plugins.Common/Actions/SendOpenAiPrompt.cs"
			},
			"rag": {
				"description": "The SendOpenAiPrompt plugin enables automation workflows to send messages to the OpenAI chat API, track conversation history per session, and retrieve the assistant’s reply. It supports custom system prompts, dynamic completion settings such as model selection, temperature, TopK, and TopP, and can reset to a new chat. It records token usage and extracts internal reasoning notes, ensuring detailed insights and reliable error logging for consistent workflow execution.",
				"qa": [
					{
						"question": "What is the SendOpenAiPrompt plugin and why does it matter?",
						"answer": "The SendOpenAiPrompt plugin sends user prompts to the OpenAI chat API and returns the AI response, which is essential for integrating conversational AI into automated workflows."
					},
					{
						"question": "What are the key features and primary parameters of this plugin?",
						"answer": "Key features include conversation history tracking, custom system prompts, completion settings (MaxTokens, Model, Temperature, TopK, TopP), new conversation reset, and response processing. Primary parameters include ApiKey for authentication, CompletionsUri for endpoint, MaxTokens to limit response length, Model selection, NewChat to reset history, Prompt for user text, SystemPrompt for custom instructions, Temperature, TopK, and TopP."
					},
					{
						"question": "How does this plugin integrate into automation workflows or external tools?",
						"answer": "Under context.integration.sequentialWorkflow it is defined as an ActionRuleModel task with iconProvider chat, making it available as a conversational AI action in low-code or RPA platforms. GitHub links for documentation and source code are provided under context.integration.github."
					},
					{
						"question": "What are best practices for configuring and using this plugin?",
						"answer": "Use a valid ApiKey and correct CompletionsUri, set Model and MaxTokens based on desired response length, adjust Temperature or TopK/TopP for desired creativity, use NewChat to clear history when needed, and monitor token output parameters to manage costs."
					},
					{
						"question": "What is the structure of the plugin rule definition?",
						"answer": "Each rule uses \"$type\": \"Action\", includes pluginName \"SendOpenAiPrompt\", and passes an argument string containing flags like --Prompt, --SystemPrompt, --NewChat, and optional completion settings. Optionally, a regularExpression property can filter the response."
					},
					{
						"question": "How does the plugin handle errors during execution?",
						"answer": "On error it logs exceptions to the workflow log stream, clears any partial state, and continues the workflow unless explicitly configured to stop on failure."
					},
					{
						"question": "What are the aliases for this plugin?",
						"answer": "The aliases are \"Wait\" and \"WaitForCondition\"."
					},
					{
						"question": "Who is the author of this plugin?",
						"answer": "The author is Roei Sabag, linked at https://www.linkedin.com/in/roei-sabag-247aa18/."
					},
					{
						"question": "What categories is this plugin listed under?",
						"answer": "It is listed under the OpenAI category, reflecting its role in interacting with OpenAI services."
					},
					{
						"question": "What GitHub documentation link is provided?",
						"answer": "The GitHub documentation is available at https://github.com/g4-api/g4-plugins/blob/main/docs/Actions/SendOpenAiPrompt.md."
					},
					{
						"question": "What GitHub source link is provided?",
						"answer": "The GitHub source code is available at https://github.com/g4-api/g4-plugins/blob/main/src/G4.Plugins.Common/Actions/SendOpenAiPrompt.cs."
					},
					{
						"question": "What is the key for this plugin?",
						"answer": "The key is SendOpenAiPrompt, which uniquely identifies this plugin in the system."
					},
					{
						"question": "What is the manifestVersion?",
						"answer": "The manifestVersion is 4, indicating the version of the manifest schema used."
					},
					{
						"question": "What outputParameters does this plugin return?",
						"answer": "It returns OpenAiCompletionTokens (number of response tokens), OpenAiPromptTokens (number of prompt tokens), OpenAiSystemResponse (filtered AI reply as Base64 string), OpenAiTotalTokens (total tokens used), and OpenAiThink (Base64-encoded internal reasoning notes)."
					},
					{
						"question": "What parameters does this plugin accept?",
						"answer": "It accepts ApiKey (string, required), CompletionsUri (string, default OpenAI endpoint), MaxTokens (number), Model (string, default gpt-4.1-mini), NewChat (switch), Prompt (string), SystemPrompt (string), Temperature (number), TopK (number), and TopP (number)."
					},
					{
						"question": "Which platforms does this plugin support?",
						"answer": "It supports Any platform, making it usable across different operating systems and environments."
					},
					{
						"question": "What is the pluginType?",
						"answer": "The pluginType is Action, indicating it performs an operation within the automation workflow."
					},
					{
						"question": "What properties define how the plugin operates?",
						"answer": "Properties include argument (string or expression for flags and prompt text) and regularExpression (regex for filtering the response)."
					},
					{
						"question": "What protocol settings does the manifest specify?",
						"answer": "Protocol settings show apiDocumentation as https://platform.openai.com/docs/api-reference/chat/create and w3c as None, indicating use of the OpenAI API reference."
					},
					{
						"question": "What summary is provided for this plugin?",
						"answer": "The summary describes that SendOpenAiPrompt sends messages to OpenAI’s chat service, tracks conversation history, supports custom prompts and completion options, and extracts hidden \"think\" notes along with the final answer."
					},
					{
						"question": "What does the entity section define?",
						"answer": "The entity section defines output fields: CompletionTokens (number), PromptTokens (number), SystemResponse (string), TotalTokens (number), and Think (Base64-encoded string)."
					},
					{
						"question": "What examples are provided in the manifest?",
						"answer": "Examples include starting a new chat with a custom system prompt, sending a prompt with custom temperature and token limits, extracting content before a <think> tag using regex, sending a prompt with TopK, and sending a prompt with TopP to a custom Open-WebUI endpoint."
					}
				]
			},
			"sequentialWorkflow": {
				"$type": "Action",
				"componentType": "task",
				"iconProvider": "chat",
				"model": "ActionRuleModel"
			}
		}
	},
	"description": [
		"### Purpose",
		"",
		"The SendOpenAiPrompt plugin sends user messages to the OpenAI chat API and returns the assistant’s reply. It keeps track of conversation history for each session and lets you set a custom starting message if needed. It also records token usage and any internal reasoning steps so you can use those details later.",
		"",
		"### Key Features and Functionality",
		"",
		"| Feature                | Description                                                                                          |",
		"|------------------------|------------------------------------------------------------------------------------------------------|",
		"| Conversation History   | Keep track of user and system messages for each session.                                             |",
		"| Custom System Prompt   | Let users supply a starting instruction, or default to a helpful assistant message.                  |",
		"| Completion Settings    | Control response length and behavior with parameters like MaxTokens, Model, Temperature, TopK, TopP. |",
		"| New Conversation Reset | Clear all past messages when a new chat is requested.                                                |",
		"| Response Processing    | Pull out and store any hidden “think” steps or parts of the response for later use.                  |",
		"",
		"### Usages in RPA",
		"",
		"| Use Case               | Description                                                                                 |",
		"|------------------------|---------------------------------------------------------------------------------------------|",
		"| User Support Chatbot   | Let a robot chat with people, answer questions, or gather information in real time.         |",
		"| Dynamic Form Filling   | Ask the API how to fill web forms and use that guidance in an automated workflow.           |",
		"| Email Draft Generation | Automatically generate or suggest email drafts based on user prompts.                       |",
		"| Content Summarization  | Send text to the API to get a quick summary and feed that back into other automation steps. |",
		"",
		"### Usages in Automation Testing",
		"",
		"| Use Case                  | Description                                                                                 |",
		"|---------------------------|---------------------------------------------------------------------------------------------|",
		"| Test Data Generation      | Ask the API to create sample inputs (names, addresses, etc.) for use in automated tests.    |",
		"| Response Validation       | Compare the API’s reply to expected patterns to check if the integration works correctly.   |",
		"| Error Handling Simulation | Trigger error messages from the API to test how the system handles bad or unexpected input. |",
		"| Performance Tracking      | Track token usage over time to make sure API calls stay within expected limits.             |"
	],
	"entity": [
		{
			"description": [
				"Count of tokens produced by the AI’s response.",
				"It shows how long the reply is in token units.",
				"Monitoring completion token usage helps manage resource consumption."
			],
			"name": "CompletionTokens",
			"type": "Number"
		},
		{
			"description": [
				"Count of tokens included in all messages sent as the prompt.",
				"It shows how many token units the request consumed.",
				"Tracking prompt token usage helps control input size."
			],
			"name": "PromptTokens",
			"type": "Number"
		},
		{
			"description": [
				"Final response after applying pattern filtering.",
				"Filtered content ensures only the intended output is returned."
			],
			"name": "SystemResponse",
			"type": "String"
		},
		{
			"description": [
				"Total tokens used for both the request prompt and the response.",
				"It indicates combined token usage for the API call.",
				"Monitoring total tokens helps manage costs and limits."
			],
			"name": "TotalTokens",
			"type": "Number"
		},
		{
			"description": [
				"Base64-encoded content extracted from internal reasoning notes.",
				"Decoding reveals the AI’s hidden annotations and thought steps.",
				"These insights help understand how the AI arrived at its answer."
			],
			"name": "Think",
			"type": "String"
		}
	],
	"examples": [
		{
			"context": {
				"annotations": {
					"edge_cases": [
						"API key missing",
						"Malformed API response",
						"Network timeout",
						"OpenAI service error",
						"Unexpected response format"
					],
					"expected_result": "A new chat is initialized with the specified system prompt, and the tutor’s explanation is returned as text.",
					"notes": "Responses are returned as JSON so downstream logic can parse and display the tutor’s explanation.",
					"use_case": "openai_newchat_system_prompt",
					"version": "1.0"
				},
				"labels": [
					"ChatInitialization",
					"OpenAIRequest"
				]
			},
			"description": [
				"### Start a New Chat with a Custom System Prompt",
				"",
				"This example demonstrates how to start a new conversation with a custom system prompt and then send a user prompt.",
				"It uses the `SendOpenAiPrompt` plugin with the flags `--NewChat`, `--SystemPrompt:You are a math tutor.`, and `--Prompt:Explain the Pythagorean theorem.`.",
				"Values are returned as JSON for downstream processing."
			],
			"rule": {
				"$type": "Action",
				"argument": "{{$ --NewChat --SystemPrompt:You are a math tutor. --Prompt:Explain the Pythagorean theorem. --ApiKey:YOUR_API_KEY}}",
				"pluginName": "SendOpenAiPrompt"
			}
		},
		{
			"context": {
				"annotations": {
					"edge_cases": [
						"API key missing",
						"Malformed API response",
						"Network timeout",
						"OpenAI service error",
						"Unexpected response format"
					],
					"expected_result": "A joke is generated with the specified temperature and token limit, and returned as text.",
					"notes": "Responses are returned as JSON so downstream logic can parse and display the joke.",
					"use_case": "openai_prompt_custom_parameters",
					"version": "1.0"
				},
				"labels": [
					"OpenAIRequest",
					"ParameterSetting"
				]
			},
			"description": [
				"### Send a Prompt with Custom Temperature and Token Limits",
				"",
				"This example demonstrates how to send a prompt with specific parameters for temperature and token limits.",
				"It uses the `SendOpenAiPrompt` plugin with the flags `--Prompt:Tell a joke.`, `--Temperature:0.7`, `--MaxTokens:100`, and `--Model:gpt-3.5-turbo`.",
				"Values are returned as JSON for downstream processing."
			],
			"rule": {
				"$type": "Action",
				"argument": "{{$ --Prompt:Tell a joke. --Temperature:0.7 --MaxTokens:100 --Model:gpt-3.5-turbo --ApiKey:YOUR_API_KEY}}",
				"pluginName": "SendOpenAiPrompt"
			}
		},
		{
			"context": {
				"annotations": {
					"edge_cases": [
						"API key missing",
						"Malformed API response",
						"Network timeout",
						"OpenAI service error",
						"Regex match occurs when it shouldn't",
						"Unexpected response format"
					],
					"expected_result": "The AI response is filtered so that only the text before any `<think>` tag is extracted and returned as a string.",
					"notes": "Extracted values are converted to strings so downstream logic can handle only the relevant portion of the response.",
					"use_case": "openai_response_regex_extraction",
					"version": "1.0"
				},
				"labels": [
					"OpenAIRequest",
					"RegexValidation"
				]
			},
			"description": [
				"### Extract Content Before `<think>` Tag from AI Response",
				"",
				"This example demonstrates how to send a prompt to an AI endpoint and use a regular expression to extract only the content appearing before a `<think>` tag in the response.",
				"It uses the `SendOpenAiPrompt` plugin with the flags `--Prompt:Analyze the following data.` and `--ApiKey:YOUR_API_KEY`, then applies the regular expression `^[^<]*` to the response text.",
				"A regular expression `^[^<]*` is applied to capture all characters up to (but not including) the first `<` character, ensuring that only the text preceding `<think>` is returned.",
				"Values are returned as strings for downstream processing."
			],
			"rule": {
				"$type": "Action",
				"argument": "{{$ --Prompt:Analyze the following data. --ApiKey:YOUR_API_KEY }}",
				"pluginName": "SendOpenAiPrompt",
				"regularExpression": "^[^<]*"
			}
		},
		{
			"context": {
				"annotations": {
					"edge_cases": [
						"API key missing",
						"Malformed API response",
						"Network timeout",
						"OpenAI service error",
						"Unexpected response format"
					],
					"expected_result": "A movie recommendation is generated with candidate tokens limited by TopK, and returned as JSON.",
					"notes": "Responses are returned as JSON so downstream logic can parse and display the recommendation.",
					"use_case": "openai_prompt_topk",
					"version": "1.0"
				},
				"labels": [
					"OpenAIRequest",
					"ParameterSetting"
				]
			},
			"description": [
				"### Send a Prompt with TopK to Limit Candidate Tokens",
				"",
				"This example demonstrates how to send a prompt with a TopK parameter to limit candidate tokens in the AI response.",
				"It uses the `SendOpenAiPrompt` plugin with the flags `--Prompt:Recommend a movie.`, `--TopK:3`, and `--ApiKey:YOUR_API_KEY`.",
				"Values are returned as JSON for downstream processing."
			],
			"rule": {
				"$type": "Action",
				"argument": "{{$ --Prompt:Recommend a movie. --TopK:3 --ApiKey:YOUR_API_KEY }}",
				"pluginName": "SendOpenAiPrompt"
			}
		},
		{
			"context": {
				"annotations": {
					"edge_cases": [
						"API key missing",
						"Malformed API response",
						"Network timeout",
						"OpenAI service error",
						"Unexpected response format"
					],
					"expected_result": "A summary is generated by the Open-WebUI endpoint with TopP applied, and returned as JSON.",
					"notes": "Responses are returned as JSON so downstream logic can parse and display the summary.",
					"use_case": "openai_prompt_topp",
					"version": "1.0"
				},
				"labels": [
					"OpenAIRequest",
					"ParameterSetting"
				]
			},
			"description": [
				"### Send a Prompt with TopP to a Custom Open-WebUI Endpoint",
				"",
				"This example demonstrates how to send a prompt with a TopP parameter to a custom Open-WebUI endpoint rather than the OpenAI API.",
				"It uses the `SendOpenAiPrompt` plugin with the flags `--Prompt:Summarize this article.`, `--CompletionsUri:http://localhost:3000/api/chat/completions`, `--TopP:0.8`, and `--ApiKey:YOUR_API_KEY`.",
				"Values are returned as JSON for downstream processing."
			],
			"rule": {
				"$type": "Action",
				"argument": "{{$ --Prompt:Summarize this article. --CompletionsUri:http://localhost:3000/api/chat/completions --TopP:0.8 --ApiKey:YOUR_API_KEY}}",
				"pluginName": "SendOpenAiPrompt"
			}
		}
	],
	"key": "SendOpenAiPrompt",
	"manifestVersion": 4,
	"outputParameters": [
		{
			"description": [
				"Count of tokens produced by the AI’s response.",
				"It shows how long the reply is in token units.",
				"Monitoring completion token usage helps manage resource consumption."
			],
			"name": "OpenAiCompletionTokens",
			"type": "Number"
		},
		{
			"description": [
				"Count of tokens included in all messages sent as the prompt.",
				"It shows how many token units the request consumed.",
				"Tracking prompt token usage helps control input size."
			],
			"name": "OpenAiPromptTokens",
			"type": "Number"
		},
		{
			"description": [
				"Base64-encoded final response after applying pattern filtering.",
				"Decoding yields the clean reply that the AI generated.",
				"Filtered content ensures only the intended output is returned."
			],
			"name": "OpenAiSystemResponse",
			"type": "String"
		},
		{
			"description": [
				"Total tokens used for both the request prompt and the response.",
				"It indicates combined token usage for the API call.",
				"Monitoring total tokens helps manage costs and limits."
			],
			"name": "OpenAiTotalTokens",
			"type": "Number"
		},
		{
			"description": [
				"Base64-encoded content extracted from internal reasoning notes.",
				"Decoding reveals the AI’s hidden annotations and thought steps.",
				"These insights help understand how the AI arrived at its answer."
			],
			"name": "OpenAiThink",
			"type": "String"
		}
	],
	"parameters": [
		{
			"description": [
				"A secret value that lets you authenticate requests to OpenAI.",
				"It ensures the system can perform actions on your behalf.",
				"Keeping a valid key protects your account."
			],
			"mandatory": true,
			"name": "ApiKey",
			"type": "String"
		},
		{
			"default": "https://api.openai.com/v1/chat/completions",
			"description": [
				"Address used for sending chat completion requests.",
				"The default points to OpenAI’s official chat endpoint."
			],
			"mandatory": false,
			"name": "CompletionsUri",
			"type": "String"
		},
		{
			"description": [
				"A number that limits how long the response can be.",
				"It controls how many tokens the reply can include."
			],
			"mandatory": false,
			"name": "MaxTokens",
			"type": "Number"
		},
		{
			"default": "gpt-4.1-mini",
			"description": [
				"Specifies which version of OpenAI to generate responses.",
				"Defaults to gpt-4.1-mini when not provided.",
				"Choosing a higher-capability model can produce more detailed answers."
			],
			"mandatory": false,
			"name": "Model",
			"type": "String"
		},
		{
			"description": [
				"Clears past messages and starts a fresh conversation when enabled.",
				"Use this to avoid carrying over context from previous interactions.",
				"Turning it on makes each prompt treated independently."
			],
			"mandatory": false,
			"name": "NewChat",
			"type": "Switch"
		},
		{
			"description": [
				"Text sent to the assistant for generating a response.",
				"The content guides the assistant on what to reply.",
				"Providing a clear message leads to more relevant answers."
			],
			"mandatory": false,
			"name": "Prompt",
			"type": "String"
		},
		{
			"description": [
				"A starting instruction that guides the conversation’s tone and focus.",
				"It appears before any user messages to set context for the AI.",
				"Using a clear prompt helps the AI understand what style or information to prioritize."
			],
			"mandatory": false,
			"name": "SystemPrompt",
			"type": "String"
		},
		{
			"description": [
				"Adjusts how creative or predictable the AI’s response will be.",
				"Higher values produce more varied and unexpected outputs.",
				"Lower values keep replies more focused and consistent."
			],
			"mandatory": false,
			"name": "Temperature",
			"type": "Number"
		},
		{
			"description": [
				"Limits token choices to the top candidates when generating a reply.",
				"Smaller values make the AI pick from fewer options, making output more deterministic.",
				"Larger values allow more variety in word selection."
			],
			"mandatory": false,
			"name": "TopK",
			"type": "Number"
		},
		{
			"description": [
				"Not available directly from OpenAI and used by other tools to control which words the AI picks.",
				"It sets a limit so words are chosen only until their combined chances reach this value.",
				"Lower numbers force the AI to pick only the most likely words, making output more predictable.",
				"Higher numbers let the AI include more word options, making responses more varied."
			],
			"mandatory": false,
			"name": "TopP",
			"type": "Number"
		}
	],
	"platforms": [
		"Any"
	],
	"pluginType": "Action",
	"properties": [
		{
			"description": [
				"Template expression to pass parameters and prompt text."
			],
			"mandatory": false,
			"name": "argument",
			"type": "String|Expression"
		},
		{
			"default": "(?s).*",
			"description": [
				"Regular expression to extract the desired portion of the system response."
			],
			"mandatory": false,
			"name": "regularExpression",
			"type": "Regex"
		}
	],
	"protocol": {
		"apiDocumentation": "https://platform.openai.com/docs/api-reference/chat/create",
		"w3c": "None"
	},
	"summary": [
		"SendOpenAiPrompt sends your messages to OpenAI's chat service and shows the replies while keeping track of each person's past messages.",
		"You can give it a custom starting instruction, set options like how long the answer should be, which version to use, and how creative it should be, and start a new chat when you want.",
		"It also picks out any hidden `think` notes in the reply and saves both those notes and the final answer for later use."
	]
}
