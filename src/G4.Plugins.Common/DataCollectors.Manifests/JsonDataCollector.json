{
	"author": {
		"link": "https://www.linkedin.com/in/roei-sabag-247aa18/",
		"name": "Roei Sabag"
	},
	"categories": [
		"DataManagement"
	],
	"context": {
		"integration": {
			"github": {
				"document": "https://github.com/g4-api/g4-plugins/blob/main/docs/DataCollectors/JsonDataCollector.md",
				"source": "https://github.com/g4-api/g4-plugins/blob/main/src/G4.Plugins.Common/DataCollectors/JsonDataCollector.cs"
			},
			"rag": {
				"description": "JsonDataCollector captures extraction-rule outputs and writes them into a JSON file wrapped in an array, supporting both streaming and bulk modes. It hooks into extraction rules, automates file creation and array management, and provides flexible parameters to ensure reliable data integration with JSON-based APIs, databases, and downstream workflows.",
				"qa": [
					{
						"question": "What is the JsonDataCollector plugin and why does it matter?",
						"answer": "JsonDataCollector converts extracted data items into JSON objects and writes them to a file, enabling seamless integration with APIs, databases, and other JSON-based consumers."
					},
					{
						"question": "How do its main capabilities work and what are its primary parameters?",
						"answer": "It embeds into extraction rules via a dataCollector object. The main parameters are ForEntity (true for streaming each object, false for bulk write), Source (the JSON file path), and Type (must be JsonDataCollector)."
					},
					{
						"question": "How does JsonDataCollector embed into external tools or workflow designers?",
						"answer": "In workflow definitions it appears as a DataCollector step. You specify it under an extraction rule with dataCollector settings, allowing low-code design tools to configure file path, streaming mode, and array management without custom coding."
					},
					{
						"question": "What are recommended configuration patterns and troubleshooting tips?",
						"answer": "Use ForEntity=true for large or continuous streams to avoid high memory use. For smaller batches, set ForEntity=false to reduce I/O calls. Verify XPath and regex rules to prevent missing or malformed objects, and ensure file paths are writable."
					},
					{
						"question": "How are action or rule definitions structured for this plugin?",
						"answer": "You define an Extraction rule ($type “Extraction”) with a dataCollector object containing ForEntity, Source, and Type fields. Nested rule entries ($type “Content”) include key, onElement, and regularExpression to shape each JSON object."
					},
					{
						"question": "How does the plugin record failures and control workflow behavior on error?",
						"answer": "On errors it adds an exception to the response and log stream. By default the workflow continues unless you configure it to stop on error in your automation designer."
					},
					{
						"question": "What is the manifestVersion?",
						"answer": "The manifestVersion is 4, indicating the schema iteration the plugin adheres to for property and parameter definitions."
					},
					{
						"question": "Who is the author?",
						"answer": "The author is Roei Sabag (https://www.linkedin.com/in/roei-sabag-247aa18/)."
					},
					{
						"question": "What are the categories?",
						"answer": "This plugin is categorized under DataManagement, reflecting its role in collecting and organizing data."
					},
					{
						"question": "What platforms does it support?",
						"answer": "The plugin supports Any platform, making it compatible with Windows, Linux, and other environments."
					},
					{
						"question": "What is the pluginType?",
						"answer": "The pluginType is DataCollector, indicating it focuses on gathering and writing data outputs."
					},
					{
						"question": "What is the key for this plugin?",
						"answer": "The key is JsonDataCollector, which is used to reference this plugin in workflow definitions."
					},
					{
						"question": "What is the summary?",
						"answer": "“The JsonDataCollector plugin serializes extraction outputs into a JSON file, wrapping records in an array. Supports streaming writes for large data sets or bulk writes for smaller runs. Ideal for downstream JSON-based systems, APIs, and analytics.”"
					},
					{
						"question": "What properties are defined?",
						"answer": "Properties include ForEntity (Boolean, optional, toggles streaming vs buffered write), Source (String, mandatory, file path), and Type (DataCollector, mandatory, must be JsonDataCollector)."
					},
					{
						"question": "What parameters are defined?",
						"answer": "This plugin does not define a separate parameters array and relies on its properties section for configuration."
					},
					{
						"question": "What protocol information is included?",
						"answer": "Protocol settings are apiDocumentation: None and w3c: None, indicating no external API or W3C schema references."
					},
					{
						"question": "Where can I find the GitHub documentation?",
						"answer": "Documentation is available at https://github.com/g4-api/g4-plugins/blob/main/docs/DataCollectors/JsonDataCollector.md"
					},
					{
						"question": "Where can I find the source code?",
						"answer": "Source code is available at https://github.com/g4-api/g4-plugins/blob/main/src/G4.Plugins.Common/DataCollectors/JsonDataCollector.cs"
					}
				]
			}
		}
	},
	"description": [
		"### Purpose",
		"",
		"JsonDataCollector captures the output of your extraction rules and writes it into a JSON file. It opens or creates the specified file, wraps records in a JSON array, and either appends each object as it’s extracted or writes the full array at the end of the run. This format simplifies integration with APIs, databases, and other JSON-based consumers.",
		"",
		"### Key Features and Functionality",
		"",
		"| Feature                 | Description                                                                                 |",
		"|-------------------------|---------------------------------------------------------------------------------------------|",
		"| Extraction Integration  | Hooks into your extraction rules so every item is automatically turned into a JSON object. |",
		"| Write Modes             | Supports streaming (`ForEntity=true`) or bulk writes (`ForEntity=false`) at end of run.     |",
		"| Array Management        | Automatically opens and closes the JSON array wrapper, ensuring valid JSON output.         |",
		"",
		"### Usages in RPA",
		"",
		"| Use Case               | Description                                                                    |",
		"|------------------------|--------------------------------------------------------------------------------|",
		"| Web Scraping           | Serializes scraped item lists into JSON for API ingestion or analytics.        |",
		"| Real-Time Data Capture | Streams each transaction or record immediately for monitoring dashboards.      |",
		"| Data Aggregation       | Collects outputs from multiple sources into one unified JSON document.         |",
		"| System Interchange     | Produces JSON files that other services or microservices can consume directly. |",
		"",
		"### Usages in Automation Testing",
		"",
		"| Use Case           | Description                                                                          |",
		"|--------------------|--------------------------------------------------------------------------------------|",
		"| Test Result Export | Records pass/fail status as JSON objects for CI systems or custom reporting tools.   |",
		"| Metrics Collection | Captures timing, resource usage, and custom metrics in JSON for downstream analysis. |"
	],
	"examples": [
		{
			"context": {
				"annotations": {
					"edge_cases": [
						"Element missing",
						"XPath mismatch",
						"Regex failure",
						"Malformed DOM",
						"Unexpected whitespace"
					],
					"expected_result": "Each record is appended as a JSON object in the array in real time.",
					"notes": "Streaming writes avoid high memory use for large pages. The plugin opens the file, writes `[` once, then for each entity writes `<object>,`, and finally closes with `]` on completion.",
					"use_case": "hotel_to_json_streaming",
					"version": "1.0"
				},
				"labels": [
					"ElementText",
					"Extraction",
					"FileWrite",
					"JsonExtraction"
				]
			},
			"description": [
				"### Stream Hotel Locations to JSON",
				"",
				"This example extracts the `Location` text from each `<div class='hotel'>` and streams it as a JSON object into `DataFile.json` immediately.",
				"It uses `extractionScope: \"Elements\"` with XPath `//div[@class='hotel']`, applies a nested Content rule to the `<p>` starting with `Location:`, and sets `ForEntity: true` for streaming."
			],
			"rule": {
				"$type": "Extraction",
				"dataCollector": {
					"ForEntity": true,
					"Source": "DataFile.json",
					"Type": "JsonDataCollector"
				},
				"extractionScope": "Elements",
				"onElement": "//div[@class='hotel']",
				"rules": [
					{
						"$type": "Content",
						"key": "Location",
						"onElement": ".//p[starts-with(.,'Location:')]",
						"regularExpression": "(?<=\\w+:).*"
					}
				]
			}
		},
		{
			"context": {
				"annotations": {
					"edge_cases": [
						"Element missing",
						"XPath mismatch",
						"Regex failure",
						"Malformed DOM",
						"Unexpected whitespace"
					],
					"expected_result": "All records are collected and written as a single JSON array at the end.",
					"notes": "Bulk write minimizes I/O calls by buffering records in memory until completion.",
					"use_case": "hotel_to_json_bulk",
					"version": "1.0"
				},
				"labels": [
					"ElementText",
					"Extraction",
					"FileWrite",
					"JsonExtraction"
				]
			},
			"description": [
				"### Bulk Hotel Locations to JSON",
				"",
				"This example extracts all `Location` values from `<div class='hotel'>` elements and writes them in one JSON array to `DataFile.json` at the end.",
				"It sets `ForEntity: false` so the plugin buffers all objects and writes them in one operation."
			],
			"rule": {
				"$type": "Extraction",
				"dataCollector": {
					"ForEntity": false,
					"Source": "DataFile.json",
					"Type": "JsonDataCollector"
				},
				"extractionScope": "Elements",
				"onElement": "//div[@class='hotel']",
				"rules": [
					{
						"$type": "Content",
						"key": "Location",
						"onElement": ".//p[starts-with(.,'Location:')]",
						"regularExpression": "(?<=\\w+:).*"
					}
				]
			}
		}
	],
	"key": "JsonDataCollector",
	"manifestVersion": 4,
	"platforms": [
		"Any"
	],
	"pluginType": "DataCollector",
	"properties": [
		{
			"description": [
				"Determines whether each record is streamed immediately (`true`) or buffered until the end (`false`)."
			],
			"mandatory": false,
			"name": "ForEntity",
			"type": "Boolean"
		},
		{
			"description": [
				"Path to the JSON file to write. The file is created if it does not exist."
			],
			"mandatory": true,
			"name": "Source",
			"type": "String"
		},
		{
			"description": [
				"Must be set to `JsonDataCollector` for this plugin."
			],
			"mandatory": true,
			"name": "Type",
			"type": "DataCollector"
		}
	],
	"protocol": {
		"apiDocumentation": "None",
		"w3c": "None"
	},
	"summary": [
		"The JsonDataCollector plugin serializes extraction outputs into a JSON file, wrapping records in an array.",
		"Supports streaming writes for large data sets or bulk writes for smaller runs.",
		"Ideal for downstream JSON-based systems, APIs, and analytics."
	]
}
